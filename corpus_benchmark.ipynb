{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd38f807",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_repo_benchmark = \"./data/corpus_benchmark\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70c9a47",
   "metadata": {},
   "source": [
    "### renommer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c45e0046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path_repo_benchmark = Path(\"./data/corpus_benchmark\")\n",
    "\n",
    "for p in path_repo_benchmark.glob(\"Copy of FRA*.jpg\"):\n",
    "    new_name = p.name.replace(\"Copy of \", \"\", 1)\n",
    "    target = p.with_name(new_name)\n",
    "    p.rename(target)\n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c7213c",
   "metadata": {},
   "source": [
    "### créer jsonl de validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c76a573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok -> data/corpus_benchmark.jsonl count: 50\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "path_repo_benchmark = Path(\"./data/corpus_benchmark\")\n",
    "out_path = Path(\"data/corpus_benchmark.jsonl\")\n",
    "\n",
    "items = []\n",
    "for p in sorted(path_repo_benchmark.glob(\"FRA*.jpg\")):\n",
    "    items.append({\n",
    "        \"nom_image\": p.name,\n",
    "        \"raw_text\": \"\",\n",
    "        \"city\": \"\",\n",
    "        \"hamlet\": \"\",\n",
    "        \"monument\": \"\",\n",
    "        \"other\": \"\"\n",
    "    })\n",
    "\n",
    "with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for obj in items:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"ok ->\", out_path, \"count:\", len(items))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9741ee7f",
   "metadata": {},
   "source": [
    "# corpus test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f6e461d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path = Path(\"data/test20/corpus_test20\")\n",
    "\n",
    "for p in path.iterdir():\n",
    "    if p.is_file() and p.name.startswith(\"copy_\"):\n",
    "        new_name = p.name.replace(\"copy_\", \"\", 1)\n",
    "        p.rename(p.with_name(new_name))\n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb39f36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok -> data/test20/metadata_test20.json rows: 20\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "path_meta = Path(\"data/metadata.json\")\n",
    "path_trans = Path(\"data/test20/transcriptions_coherentes_final.json\")\n",
    "out_path = Path(\"data/test20/metadata_test20.json\")\n",
    "\n",
    "meta = json.loads(path_meta.read_text(encoding=\"utf-8\"))\n",
    "trans = json.loads(path_trans.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "wanted = {t.get(\"file_name\") for t in trans if t.get(\"file_name\")}\n",
    "\n",
    "file_names = meta.get(\"File name\", [])\n",
    "keep_idx = [i for i, name in enumerate(file_names) if name in wanted]\n",
    "\n",
    "filtered = {}\n",
    "for key, values in meta.items():\n",
    "    if isinstance(values, list):\n",
    "        filtered[key] = [values[i] for i in keep_idx]\n",
    "    else:\n",
    "        filtered[key] = values\n",
    "\n",
    "out_path.write_text(json.dumps(filtered, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(\"ok ->\", out_path, \"rows:\", len(keep_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b65e814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok -> data/test20/metadata_test20_enrichi.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "path_meta = Path(\"data/test20/metadata_test20.json\")\n",
    "path_trans = Path(\"data/test20/transcriptions_coherentes_final.json\")\n",
    "out_path = Path(\"data/test20/metadata_test20_enrichi.json\")\n",
    "\n",
    "meta = json.loads(path_meta.read_text(encoding=\"utf-8\"))\n",
    "trans = json.loads(path_trans.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "# Index par nom de fichier\n",
    "trans_by_name = {t.get(\"file_name\"): t for t in trans}\n",
    "\n",
    "file_names = meta.get(\"File name\", [])\n",
    "n = len(file_names)\n",
    "\n",
    "# toutes les clés présentes dans transcriptions (sauf file_name)\n",
    "trans_keys = set()\n",
    "for t in trans:\n",
    "    for k in t.keys():\n",
    "        if k != \"file_name\":\n",
    "            trans_keys.add(k)\n",
    "\n",
    "# colonnes cibles (prefixées pour éviter collision)\n",
    "col_map = {k: f\"trans_{k}\" for k in sorted(trans_keys)}\n",
    "\n",
    "# initialise les colonnes\n",
    "for col in col_map.values():\n",
    "    if col not in meta:\n",
    "        meta[col] = [\"\"] * n\n",
    "\n",
    "# remplissage\n",
    "for i, fname in enumerate(file_names):\n",
    "    t = trans_by_name.get(fname)\n",
    "    if not t:\n",
    "        continue\n",
    "    for src_key, dst_key in col_map.items():\n",
    "        meta[dst_key][i] = t.get(src_key, \"\")\n",
    "\n",
    "out_path.write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(\"ok ->\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7420aefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok -> data/test20/metadata_test20_enrichi.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "in_path = Path(\"data/test20/metadata_test20_enrichi.json\")\n",
    "out_path = Path(\"data/test20/metadata_test20_enrichi.csv\")\n",
    "\n",
    "data = json.loads(in_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "fields = list(data.keys())\n",
    "n = len(data.get(\"File name\", []))\n",
    "\n",
    "with out_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(fields)\n",
    "    for i in range(n):\n",
    "        row = []\n",
    "        for k in fields:\n",
    "            v = data.get(k, \"\")\n",
    "            if isinstance(v, list):\n",
    "                row.append(v[i] if i < len(v) else \"\")\n",
    "            else:\n",
    "                row.append(v)\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"ok ->\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc023c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok -> data/metadata_monu_52.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "in_path = Path(\"data/metadata_monu.json\")\n",
    "out_path = Path(\"data/metadata_monu_52.json\")\n",
    "\n",
    "data = json.loads(in_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "# longueur basée sur File name\n",
    "n = min(52, len(data.get(\"File name\", [])))\n",
    "\n",
    "# supprimer colonnes\n",
    "data.pop(\"longitude\", None)\n",
    "data.pop(\"latitude\", None)\n",
    "\n",
    "# tronquer toutes les colonnes listes\n",
    "for k, v in data.items():\n",
    "    if isinstance(v, list):\n",
    "        data[k] = v[:n]\n",
    "\n",
    "out_path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(\"ok ->\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d8024f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok -> data/metadata_monu_52.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "in_path = Path(\"data/metadata_monu_52.json\")\n",
    "out_path = Path(\"data/metadata_monu_52.csv\")\n",
    "\n",
    "data = json.loads(in_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "fields = list(data.keys())\n",
    "n = len(data.get(\"File name\", []))\n",
    "\n",
    "with out_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(fields)\n",
    "    for i in range(n):\n",
    "        row = []\n",
    "        for k in fields:\n",
    "            v = data.get(k, \"\")\n",
    "            if isinstance(v, list):\n",
    "                row.append(v[i] if i < len(v) else \"\")\n",
    "            else:\n",
    "                row.append(v)\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"ok ->\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efc2a21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok -> data/metadata.json and data/metadata.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "path_json = Path(\"data/metadata.json\")\n",
    "path_csv = Path(\"data/metadata.csv\")\n",
    "\n",
    "remove_cols = {\n",
    "    \"City\", \"Raw text\", \"Hamlet\", \"Monument\", \"Other\", \"coordonnées\"\n",
    "}\n",
    "rename_from = \"code_insee\"\n",
    "rename_to = \"code_insee_commune\"\n",
    "\n",
    "# --- JSON ---\n",
    "data = json.loads(path_json.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "# remove columns\n",
    "for col in remove_cols:\n",
    "    data.pop(col, None)\n",
    "\n",
    "# rename column\n",
    "if rename_from in data:\n",
    "    data[rename_to] = data.pop(rename_from)\n",
    "\n",
    "path_json.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# --- CSV ---\n",
    "with path_csv.open(\"r\", encoding=\"utf-8\") as f_in:\n",
    "    reader = csv.DictReader(f_in)\n",
    "    fieldnames = []\n",
    "    for fn in reader.fieldnames:\n",
    "        if fn in remove_cols:\n",
    "            continue\n",
    "        if fn == rename_from:\n",
    "            fieldnames.append(rename_to)\n",
    "        else:\n",
    "            fieldnames.append(fn)\n",
    "\n",
    "    rows = []\n",
    "    for row in reader:\n",
    "        new_row = {}\n",
    "        for k, v in row.items():\n",
    "            if k in remove_cols:\n",
    "                continue\n",
    "            if k == rename_from:\n",
    "                new_row[rename_to] = v\n",
    "            else:\n",
    "                new_row[k] = v\n",
    "        rows.append(new_row)\n",
    "\n",
    "with path_csv.open(\"w\", newline=\"\", encoding=\"utf-8\") as f_out:\n",
    "    writer = csv.DictWriter(f_out, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(\"ok ->\", path_json, \"and\", path_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fad9abe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement PIL (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for PIL\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e15a5081",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12405cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.9.1-cp313-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.20.2-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.13/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.13/site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Collecting fsspec>=0.8.5 (from torch)\n",
      "  Using cached fsspec-2025.12.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.13/site-packages (from jinja2->torch) (3.0.3)\n",
      "Using cached torch-2.9.1-cp313-none-macosx_11_0_arm64.whl (74.5 MB)\n",
      "Using cached fsspec-2025.12.0-py3-none-any.whl (201 kB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Downloading filelock-3.20.2-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: mpmath, sympy, fsspec, filelock, torch\n",
      "\u001b[2K   \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [sympy]^C\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce6bad22",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./vaucluse/truc.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ecc27c8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './vaucluse/truc.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[32m      3\u001b[39m         \u001b[38;5;28mprint\u001b[39m(line)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CODE/hackathon/vaucluse/.venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py:344\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    338\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    339\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    342\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './vaucluse/truc.txt'"
     ]
    }
   ],
   "source": [
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106ecf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_metadata = \"./data/metadata.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0fd0fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok -> data/metadata_output.json rows: 84\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "path_meta = Path(\"data/metadata.json\")\n",
    "out_path = Path(\"data/metadata_output.json\")\n",
    "\n",
    "# Liste des 84 cartes\n",
    "wanted = {\n",
    "    \"FRAD084_07FI137_0112.jpg\",\n",
    "    \"FRAD084_07FI129_0002.jpg\",\n",
    "    \"FRAD084_07FI139_0028.jpg\",\n",
    "    \"FRAD084_07FI050_0024_01.jpg\",\n",
    "    \"FRAD084_07FI087_0275_01.jpg\",\n",
    "    \"FRAD084_07FI007_0382.jpg\",\n",
    "    \"FRAD084_07FI139_0088.jpg\",\n",
    "    \"FRAD084_07FI007_0677.jpg\",\n",
    "    \"FRAD084_07FI007_0504.jpg\",\n",
    "    \"FRAD084_07FI007_0470.jpg\",\n",
    "    \"FRAD084_07FI058_0002_01.jpg\",\n",
    "    \"FRAD084_07FI152_0018.jpg\",\n",
    "    \"FRAD084_07FI007_0827.jpg\",\n",
    "    \"FRAD084_07FI089_0038.jpg\",\n",
    "    \"FRAD084_07FI124_0001_01.jpg\",\n",
    "    \"FRAD084_07FI087_0114.jpg\",\n",
    "    \"FRAD084_07FI007_0699.jpg\",\n",
    "    \"FRAD084_07FI007_0513.jpg\",\n",
    "    \"FRAD084_07FI097_0001_01.jpg\",\n",
    "    \"FRAD084_07FI152_0002.jpg\",\n",
    "    \"FRAD084_07FI114_0001_01.jpg\",\n",
    "    \"FRAD084_07FI152_0046.jpg\",\n",
    "    \"FRAD084_07FI056_0001_01.jpg\",\n",
    "    \"FRAD084_07FI087_0185.jpg\",\n",
    "    \"FRAD084_07FI007_0719.jpg\",\n",
    "    \"FRAD084_07FI087_0310.jpg\",\n",
    "    \"FRAD084_07FI069_0005_01.jpg\",\n",
    "    \"FRAD084_07FI007_0453.jpg\",\n",
    "    \"FRAD084_07FI007_0940.jpg\",\n",
    "    \"FRAD084_07FI138_0004.jpg\",\n",
    "    \"FRAD084_07FI087_0223_01.jpg\",\n",
    "    \"FRAD084_07FI020_0006_01.jpg\",\n",
    "    \"FRAD084_07FI007_0586.jpg\",\n",
    "    \"FRAD084_07FI087_0267_01.jpg\",\n",
    "    \"FRAD084_07FI074_0004.jpg\",\n",
    "    \"FRAD084_07FI087_0186_01.jpg\",\n",
    "    \"FRAD084_07FI007_0498.jpg\",\n",
    "    \"FRAD084_07FI129_0001.jpg\",\n",
    "    \"FRAD084_07FI007_0067.jpg\",\n",
    "    \"FRAD084_07FI139_0094.jpg\",\n",
    "    \"FRAD084_07FI031_0119.jpg\",\n",
    "    \"FRAD084_07FI087_0081_01.jpg\",\n",
    "    \"FRAD084_07FI072_0001.jpg\",\n",
    "    \"FRAD084_07FI152_0004.jpg\",\n",
    "    \"FRAD084_07FI139_0035.jpg\",\n",
    "    \"FRAD084_07FI003_0027.jpg\",\n",
    "    \"FRAD084_07FI007_0915.jpg\",\n",
    "    \"FRAD084_07FI152_0071.jpg\",\n",
    "    \"FRAD084_07FI007_0834.jpg\",\n",
    "    \"FRAD084_07FI007_0714_01.jpg\",\n",
    "    \"FRAD084_07FI007_0590.jpg\",\n",
    "    \"FRAD084_07FI087_0251_01.jpg\",\n",
    "    \"FRAD084_07FI007_1063_01.jpg\",\n",
    "    \"FRAD084_07FI124_0003.jpg\",\n",
    "    \"FRAD084_07FI007_0158.jpg\",\n",
    "    \"FRAD084_07FI089_0029_01.jpg\",\n",
    "    \"FRAD084_07FI132_0022_01.jpg\",\n",
    "    \"FRAD084_07FI007_0819.jpg\",\n",
    "    \"FRAD084_07FI087_0056.jpg\",\n",
    "    \"FRAD084_07FI007_0375_01.jpg\",\n",
    "    \"FRAD084_07FI042_0002.jpg\",\n",
    "    \"FRAD084_07FI139_0059_01.jpg\",\n",
    "    \"FRAD084_07FI152_0094.jpg\",\n",
    "    \"FRAD084_07FI041_0033.jpg\",\n",
    "    \"FRAD084_07FI007_0492_01.jpg\",\n",
    "    \"FRAD084_07FI007_0670.jpg\",\n",
    "    \"FRAD084_07FI050_0033.jpg\",\n",
    "    \"FRAD084_07FI007_0965.jpg\",\n",
    "    \"FRAD084_07FI031_0079_01.jpg\",\n",
    "    \"FRAD084_07FI087_0298.jpg\",\n",
    "    \"FRAD084_07FI139_0152.jpg\",\n",
    "    \"FRAD084_07FI087_0300_01.jpg\",\n",
    "    \"FRAD084_07FI007_0954.jpg\",\n",
    "    \"FRAD084_07FI007_0950.jpg\",\n",
    "    \"FRAD084_07FI019_0008_01.jpg\",\n",
    "    \"FRAD084_07FI137_0030_01.jpg\",\n",
    "    \"FRAD084_07FI087_0122.jpg\",\n",
    "    \"FRAD084_07FI003_0073.jpg\",\n",
    "    \"FRAD084_07FI152_0020.jpg\",\n",
    "    \"FRAD084_07FI123_0042.jpg\",\n",
    "    \"FRAD084_07FI081_0007_01.jpg\",\n",
    "    \"FRAD084_07FI007_0414.jpg\",\n",
    "    \"FRAD084_07FI017_0008.jpg\",\n",
    "    \"FRAD084_07FI088_0023_01.jpg\",\n",
    "}\n",
    "\n",
    "meta = json.loads(path_meta.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "file_names = meta.get(\"File name\", [])\n",
    "keep_idx = [i for i, name in enumerate(file_names) if name in wanted]\n",
    "\n",
    "filtered = {}\n",
    "for key, values in meta.items():\n",
    "    if isinstance(values, list):\n",
    "        filtered[key] = [values[i] for i in keep_idx]\n",
    "    else:\n",
    "        filtered[key] = values\n",
    "\n",
    "out_path.write_text(json.dumps(filtered, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(\"ok ->\", out_path, \"rows:\", len(keep_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "893a51d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok -> data/metadata_output.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "path_meta = Path(\"data/metadata_output.json\")\n",
    "path_trans = Path(\"data/out_vaucluse/transcriptions_coherentes_final.json\")\n",
    "out_path = Path(\"data/metadata_output.json\")  # overwrite\n",
    "\n",
    "meta = json.loads(path_meta.read_text(encoding=\"utf-8\"))\n",
    "trans = json.loads(path_trans.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "# index transcriptions par nom de fichier\n",
    "trans_by_name = {t.get(\"file_name\"): t for t in trans}\n",
    "\n",
    "file_names = meta.get(\"File name\", [])\n",
    "n = len(file_names)\n",
    "\n",
    "# toutes les clés de transcriptions (sauf file_name)\n",
    "trans_keys = set()\n",
    "for t in trans:\n",
    "    for k in t.keys():\n",
    "        if k != \"file_name\":\n",
    "            trans_keys.add(k)\n",
    "\n",
    "# colonnes cibles (préfixées)\n",
    "col_map = {k: f\"trans_{k}\" for k in sorted(trans_keys)}\n",
    "\n",
    "# initialise colonnes si besoin\n",
    "for col in col_map.values():\n",
    "    if col not in meta:\n",
    "        meta[col] = [\"\"] * n\n",
    "\n",
    "# remplissage\n",
    "for i, fname in enumerate(file_names):\n",
    "    t = trans_by_name.get(fname)\n",
    "    if not t:\n",
    "        continue\n",
    "    for src_key, dst_key in col_map.items():\n",
    "        meta[dst_key][i] = t.get(src_key, \"\")\n",
    "\n",
    "out_path.write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(\"ok ->\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "469b446f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok -> data/metadata_output.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "in_path = Path(\"data/metadata_output.json\")\n",
    "out_path = Path(\"data/metadata_output.csv\")\n",
    "\n",
    "data = json.loads(in_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "fields = list(data.keys())\n",
    "n = len(data.get(\"File name\", []))\n",
    "\n",
    "with out_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(fields)\n",
    "    for i in range(n):\n",
    "        row = []\n",
    "        for k in fields:\n",
    "            v = data.get(k, \"\")\n",
    "            if isinstance(v, list):\n",
    "                row.append(v[i] if i < len(v) else \"\")\n",
    "            else:\n",
    "                row.append(v)\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"ok ->\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66de5b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copié: 84\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "src_dir = Path(\"data/photos\")\n",
    "dst_dir = Path(\"data/output_vaucluse/images\")\n",
    "dst_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "files = [\n",
    "    \"FRAD084_07FI137_0112.jpg\",\n",
    "    \"FRAD084_07FI129_0002.jpg\",\n",
    "    \"FRAD084_07FI139_0028.jpg\",\n",
    "    \"FRAD084_07FI050_0024_01.jpg\",\n",
    "    \"FRAD084_07FI087_0275_01.jpg\",\n",
    "    \"FRAD084_07FI007_0382.jpg\",\n",
    "    \"FRAD084_07FI139_0088.jpg\",\n",
    "    \"FRAD084_07FI007_0677.jpg\",\n",
    "    \"FRAD084_07FI007_0504.jpg\",\n",
    "    \"FRAD084_07FI007_0470.jpg\",\n",
    "    \"FRAD084_07FI058_0002_01.jpg\",\n",
    "    \"FRAD084_07FI152_0018.jpg\",\n",
    "    \"FRAD084_07FI007_0827.jpg\",\n",
    "    \"FRAD084_07FI089_0038.jpg\",\n",
    "    \"FRAD084_07FI124_0001_01.jpg\",\n",
    "    \"FRAD084_07FI087_0114.jpg\",\n",
    "    \"FRAD084_07FI007_0699.jpg\",\n",
    "    \"FRAD084_07FI007_0513.jpg\",\n",
    "    \"FRAD084_07FI097_0001_01.jpg\",\n",
    "    \"FRAD084_07FI152_0002.jpg\",\n",
    "    \"FRAD084_07FI114_0001_01.jpg\",\n",
    "    \"FRAD084_07FI152_0046.jpg\",\n",
    "    \"FRAD084_07FI056_0001_01.jpg\",\n",
    "    \"FRAD084_07FI087_0185.jpg\",\n",
    "    \"FRAD084_07FI007_0719.jpg\",\n",
    "    \"FRAD084_07FI087_0310.jpg\",\n",
    "    \"FRAD084_07FI069_0005_01.jpg\",\n",
    "    \"FRAD084_07FI007_0453.jpg\",\n",
    "    \"FRAD084_07FI007_0940.jpg\",\n",
    "    \"FRAD084_07FI138_0004.jpg\",\n",
    "    \"FRAD084_07FI087_0223_01.jpg\",\n",
    "    \"FRAD084_07FI020_0006_01.jpg\",\n",
    "    \"FRAD084_07FI007_0586.jpg\",\n",
    "    \"FRAD084_07FI087_0267_01.jpg\",\n",
    "    \"FRAD084_07FI074_0004.jpg\",\n",
    "    \"FRAD084_07FI087_0186_01.jpg\",\n",
    "    \"FRAD084_07FI007_0498.jpg\",\n",
    "    \"FRAD084_07FI129_0001.jpg\",\n",
    "    \"FRAD084_07FI007_0067.jpg\",\n",
    "    \"FRAD084_07FI139_0094.jpg\",\n",
    "    \"FRAD084_07FI031_0119.jpg\",\n",
    "    \"FRAD084_07FI087_0081_01.jpg\",\n",
    "    \"FRAD084_07FI072_0001.jpg\",\n",
    "    \"FRAD084_07FI152_0004.jpg\",\n",
    "    \"FRAD084_07FI139_0035.jpg\",\n",
    "    \"FRAD084_07FI003_0027.jpg\",\n",
    "    \"FRAD084_07FI007_0915.jpg\",\n",
    "    \"FRAD084_07FI152_0071.jpg\",\n",
    "    \"FRAD084_07FI007_0834.jpg\",\n",
    "    \"FRAD084_07FI007_0714_01.jpg\",\n",
    "    \"FRAD084_07FI007_0590.jpg\",\n",
    "    \"FRAD084_07FI087_0251_01.jpg\",\n",
    "    \"FRAD084_07FI007_1063_01.jpg\",\n",
    "    \"FRAD084_07FI124_0003.jpg\",\n",
    "    \"FRAD084_07FI007_0158.jpg\",\n",
    "    \"FRAD084_07FI089_0029_01.jpg\",\n",
    "    \"FRAD084_07FI132_0022_01.jpg\",\n",
    "    \"FRAD084_07FI007_0819.jpg\",\n",
    "    \"FRAD084_07FI087_0056.jpg\",\n",
    "    \"FRAD084_07FI007_0375_01.jpg\",\n",
    "    \"FRAD084_07FI042_0002.jpg\",\n",
    "    \"FRAD084_07FI139_0059_01.jpg\",\n",
    "    \"FRAD084_07FI152_0094.jpg\",\n",
    "    \"FRAD084_07FI041_0033.jpg\",\n",
    "    \"FRAD084_07FI007_0492_01.jpg\",\n",
    "    \"FRAD084_07FI007_0670.jpg\",\n",
    "    \"FRAD084_07FI050_0033.jpg\",\n",
    "    \"FRAD084_07FI007_0965.jpg\",\n",
    "    \"FRAD084_07FI031_0079_01.jpg\",\n",
    "    \"FRAD084_07FI087_0298.jpg\",\n",
    "    \"FRAD084_07FI139_0152.jpg\",\n",
    "    \"FRAD084_07FI087_0300_01.jpg\",\n",
    "    \"FRAD084_07FI007_0954.jpg\",\n",
    "    \"FRAD084_07FI007_0950.jpg\",\n",
    "    \"FRAD084_07FI019_0008_01.jpg\",\n",
    "    \"FRAD084_07FI137_0030_01.jpg\",\n",
    "    \"FRAD084_07FI087_0122.jpg\",\n",
    "    \"FRAD084_07FI003_0073.jpg\",\n",
    "    \"FRAD084_07FI152_0020.jpg\",\n",
    "    \"FRAD084_07FI123_0042.jpg\",\n",
    "    \"FRAD084_07FI081_0007_01.jpg\",\n",
    "    \"FRAD084_07FI007_0414.jpg\",\n",
    "    \"FRAD084_07FI017_0008.jpg\",\n",
    "    \"FRAD084_07FI088_0023_01.jpg\",\n",
    "]\n",
    "\n",
    "missing = []\n",
    "for name in files:\n",
    "    src = src_dir / name\n",
    "    if not src.exists():\n",
    "        missing.append(name)\n",
    "        continue\n",
    "    shutil.copy2(src, dst_dir / name)\n",
    "\n",
    "print(\"copié:\", len(files) - len(missing))\n",
    "if missing:\n",
    "    print(\"introuvables:\", missing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb87164c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok -> data/out_vaucluse/data_manuel.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path(\"./data/out_vaucluse/data_manuel.json\")\n",
    "\n",
    "data = json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "for item in data:\n",
    "    if \"city\" in item:\n",
    "        item[\"city\"] = \"\"\n",
    "    if \"monument_uniformise\" in item:\n",
    "        item[\"monument_uniformise\"] = \"\"\n",
    "    if \"hamlet_uniformise\" in item:\n",
    "        item[\"hamlet_uniformise\"] = \"\"\n",
    "    item.pop(\"confidence\", None)\n",
    "\n",
    "path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(\"ok ->\", path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "88f15d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok -> data/out_vaucluse/data_manuel.json\n",
      "ok -> data/out_vaucluse/transcriptions_coherentes_final.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "paths = [\n",
    "    Path(\"data/out_vaucluse/data_manuel.json\"),\n",
    "    Path(\"data/out_vaucluse/transcriptions_coherentes_final.json\"),\n",
    "]\n",
    "\n",
    "for path in paths:\n",
    "    data = json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    data_sorted = sorted(data, key=lambda x: x.get(\"file_name\", \"\"))\n",
    "    path.write_text(json.dumps(data_sorted, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    print(\"ok ->\", path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "537bc790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok -> data/out_vaucluse/data_manuel.json\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "src = Path(\"data/out_vaucluse/transcriptions_coherentes_final.json\")\n",
    "dst = Path(\"data/out_vaucluse/data_manuel.json\")\n",
    "\n",
    "dst.write_text(src.read_text(encoding=\"utf-8\"), encoding=\"utf-8\")\n",
    "print(\"ok ->\", dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "93a03884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city: total=84 errors=4 empty_vs_full=0 mismatch=4\n",
      "monument_uniformise: total=84 errors=16 empty_vs_full=8 mismatch=8\n",
      "hamlet_uniformise: total=84 errors=7 empty_vs_full=2 mismatch=5\n",
      "hamlet_uniformise == 'Aucun lieu-dit' (manual): 70\n",
      "hamlet_uniformise == 'Aucun lieu-dit' (auto): 73\n",
      "hamlet_uniformise == 'Aucun lieu-dit' (both): 70\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import unicodedata\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "path_manual = Path(\"data/out_vaucluse/data_manuel.json\")\n",
    "path_auto = Path(\"data/out_vaucluse/transcriptions_coherentes_final.json\")\n",
    "\n",
    "def normalize(s):\n",
    "    s = (s or \"\").lower()\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "keys = [\"city\", \"monument_uniformise\", \"hamlet_uniformise\"]\n",
    "\n",
    "manual = json.loads(path_manual.read_text(encoding=\"utf-8\"))\n",
    "auto = json.loads(path_auto.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "manual_by_name = {d.get(\"file_name\"): d for d in manual}\n",
    "auto_by_name = {d.get(\"file_name\"): d for d in auto}\n",
    "\n",
    "stats = {k: {\"total\": 0, \"errors\": 0, \"empty_vs_full\": 0, \"mismatch\": 0} for k in keys}\n",
    "\n",
    "# compte \"Aucun lieu-dit\"\n",
    "target = normalize(\"Aucun lieu-dit\")\n",
    "count_manual = 0\n",
    "count_auto = 0\n",
    "count_both = 0\n",
    "\n",
    "for fname, m in manual_by_name.items():\n",
    "    a = auto_by_name.get(fname)\n",
    "    if not a:\n",
    "        continue\n",
    "\n",
    "    for k in keys:\n",
    "        m_val = normalize(m.get(k))\n",
    "        a_val = normalize(a.get(k))\n",
    "        stats[k][\"total\"] += 1\n",
    "\n",
    "        if m_val != a_val:\n",
    "            stats[k][\"errors\"] += 1\n",
    "            if (not m_val and a_val) or (m_val and not a_val):\n",
    "                stats[k][\"empty_vs_full\"] += 1\n",
    "            else:\n",
    "                stats[k][\"mismatch\"] += 1\n",
    "\n",
    "    m_h = normalize(m.get(\"hamlet_uniformise\"))\n",
    "    a_h = normalize(a.get(\"hamlet_uniformise\"))\n",
    "    if m_h == target:\n",
    "        count_manual += 1\n",
    "    if a_h == target:\n",
    "        count_auto += 1\n",
    "    if m_h == target and a_h == target:\n",
    "        count_both += 1\n",
    "\n",
    "for k in keys:\n",
    "    s = stats[k]\n",
    "    print(f\"{k}: total={s['total']} errors={s['errors']} empty_vs_full={s['empty_vs_full']} mismatch={s['mismatch']}\")\n",
    "\n",
    "print(f\"hamlet_uniformise == 'Aucun lieu-dit' (manual): {count_manual}\")\n",
    "print(f\"hamlet_uniformise == 'Aucun lieu-dit' (auto): {count_auto}\")\n",
    "print(f\"hamlet_uniformise == 'Aucun lieu-dit' (both): {count_both}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a12ccbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city: total=84 errors=4 manual_empty_auto_full=0 manual_full_auto_empty=0 mismatch=4\n",
      "monument_uniformise: total=84 errors=16 manual_empty_auto_full=8 manual_full_auto_empty=0 mismatch=8\n",
      "hamlet_uniformise: total=84 errors=7 manual_empty_auto_full=2 manual_full_auto_empty=0 mismatch=5\n",
      "hamlet_uniformise == 'Aucun lieu-dit' (manual): 70\n",
      "hamlet_uniformise == 'Aucun lieu-dit' (auto): 73\n",
      "hamlet_uniformise == 'Aucun lieu-dit' (both): 70\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import unicodedata\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "path_manual = Path(\"data/out_vaucluse/data_manuel.json\")\n",
    "path_auto = Path(\"data/out_vaucluse/transcriptions_coherentes_final.json\")\n",
    "\n",
    "def normalize(s):\n",
    "    s = (s or \"\").lower()\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "keys = [\"city\", \"monument_uniformise\", \"hamlet_uniformise\"]\n",
    "\n",
    "manual = json.loads(path_manual.read_text(encoding=\"utf-8\"))\n",
    "auto = json.loads(path_auto.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "manual_by_name = {d.get(\"file_name\"): d for d in manual}\n",
    "auto_by_name = {d.get(\"file_name\"): d for d in auto}\n",
    "\n",
    "stats = {\n",
    "    k: {\n",
    "        \"total\": 0,\n",
    "        \"errors\": 0,\n",
    "        \"manual_empty_auto_full\": 0,\n",
    "        \"manual_full_auto_empty\": 0,\n",
    "        \"mismatch\": 0,\n",
    "    }\n",
    "    for k in keys\n",
    "}\n",
    "\n",
    "# compte \"Aucun lieu-dit\"\n",
    "target = normalize(\"Aucun lieu-dit\")\n",
    "count_manual = 0\n",
    "count_auto = 0\n",
    "count_both = 0\n",
    "\n",
    "for fname, m in manual_by_name.items():\n",
    "    a = auto_by_name.get(fname)\n",
    "    if not a:\n",
    "        continue\n",
    "\n",
    "    for k in keys:\n",
    "        m_val = normalize(m.get(k))\n",
    "        a_val = normalize(a.get(k))\n",
    "        stats[k][\"total\"] += 1\n",
    "\n",
    "        if m_val != a_val:\n",
    "            stats[k][\"errors\"] += 1\n",
    "            if not m_val and a_val:\n",
    "                stats[k][\"manual_empty_auto_full\"] += 1\n",
    "            elif m_val and not a_val:\n",
    "                stats[k][\"manual_full_auto_empty\"] += 1\n",
    "            else:\n",
    "                stats[k][\"mismatch\"] += 1\n",
    "\n",
    "    m_h = normalize(m.get(\"hamlet_uniformise\"))\n",
    "    a_h = normalize(a.get(\"hamlet_uniformise\"))\n",
    "    if m_h == target:\n",
    "        count_manual += 1\n",
    "    if a_h == target:\n",
    "        count_auto += 1\n",
    "    if m_h == target and a_h == target:\n",
    "        count_both += 1\n",
    "\n",
    "for k in keys:\n",
    "    s = stats[k]\n",
    "    print(\n",
    "        f\"{k}: total={s['total']} errors={s['errors']} \"\n",
    "        f\"manual_empty_auto_full={s['manual_empty_auto_full']} \"\n",
    "        f\"manual_full_auto_empty={s['manual_full_auto_empty']} \"\n",
    "        f\"mismatch={s['mismatch']}\"\n",
    "    )\n",
    "\n",
    "print(f\"hamlet_uniformise == 'Aucun lieu-dit' (manual): {count_manual}\")\n",
    "print(f\"hamlet_uniformise == 'Aucun lieu-dit' (auto): {count_auto}\")\n",
    "print(f\"hamlet_uniformise == 'Aucun lieu-dit' (both): {count_both}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a9fde71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 villes pour lesquelles on a pas le nom de la ville sur la carte\n",
      "city: total=84 errors=9 \n",
      "monument_uniformise: total=84 errors=17 \n",
      "hamlet_uniformise: total=84 errors=7 \n",
      " \n",
      "hamlet_uniformise == 'Aucun lieu-dit' (manual): 72\n",
      "hamlet_uniformise == 'Aucun lieu-dit' (auto): 73\n",
      "hamlet_uniformise == 'Aucun lieu-dit' (both): 70\n",
      " \n",
      "monument_uniformise == 'Aucun monument' (manual): 30\n",
      "monument_uniformise == 'Aucun monument' (auto): 21\n",
      "monument_uniformise == 'Aucun monument' (both): 21\n",
      " \n",
      "city == 'Aucune ville' (manual): 5\n",
      "city == 'Aucune ville' (auto): 0\n",
      "city == 'Aucune ville' (both): 0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import unicodedata\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "path_manual = Path(\"data/out_vaucluse/data_manuel.json\")\n",
    "path_auto = Path(\"data/out_vaucluse/transcriptions_coherentes_final.json\")\n",
    "\n",
    "def normalize(s):\n",
    "    s = (s or \"\").lower()\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "keys = [\"city\", \"monument_uniformise\", \"hamlet_uniformise\"]\n",
    "\n",
    "manual = json.loads(path_manual.read_text(encoding=\"utf-8\"))\n",
    "auto = json.loads(path_auto.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "manual_by_name = {d.get(\"file_name\"): d for d in manual}\n",
    "auto_by_name = {d.get(\"file_name\"): d for d in auto}\n",
    "\n",
    "stats = {\n",
    "    k: {\n",
    "        \"total\": 0,\n",
    "        \"errors\": 0,\n",
    "        \"manual_empty_auto_full\": 0,\n",
    "        \"manual_full_auto_empty\": 0,\n",
    "        \"mismatch\": 0,\n",
    "    }\n",
    "    for k in keys\n",
    "}\n",
    "\n",
    "target_hamlet = normalize(\"Aucun lieu-dit\")\n",
    "target_monument = normalize(\"Aucun monument\")\n",
    "target_city = normalize(\"Aucune ville\")\n",
    "\n",
    "count_hamlet_manual = 0\n",
    "count_hamlet_auto = 0\n",
    "count_hamlet_both = 0\n",
    "\n",
    "count_monu_manual = 0\n",
    "count_monu_auto = 0\n",
    "count_monu_both = 0\n",
    "\n",
    "count_city_manual = 0\n",
    "count_city_auto = 0\n",
    "count_city_both = 0\n",
    "\n",
    "for fname, m in manual_by_name.items():\n",
    "    a = auto_by_name.get(fname)\n",
    "    if not a:\n",
    "        continue\n",
    "\n",
    "    for k in keys:\n",
    "        m_val = normalize(m.get(k))\n",
    "        a_val = normalize(a.get(k))\n",
    "        stats[k][\"total\"] += 1\n",
    "\n",
    "        if m_val != a_val:\n",
    "            stats[k][\"errors\"] += 1\n",
    "            if not m_val and a_val:\n",
    "                stats[k][\"manual_empty_auto_full\"] += 1\n",
    "            elif m_val and not a_val:\n",
    "                stats[k][\"manual_full_auto_empty\"] += 1\n",
    "            else:\n",
    "                stats[k][\"mismatch\"] += 1\n",
    "\n",
    "    m_h = normalize(m.get(\"hamlet_uniformise\"))\n",
    "    a_h = normalize(a.get(\"hamlet_uniformise\"))\n",
    "    if m_h == target_hamlet:\n",
    "        count_hamlet_manual += 1\n",
    "    if a_h == target_hamlet:\n",
    "        count_hamlet_auto += 1\n",
    "    if m_h == target_hamlet and a_h == target_hamlet:\n",
    "        count_hamlet_both += 1\n",
    "\n",
    "    m_m = normalize(m.get(\"monument_uniformise\"))\n",
    "    a_m = normalize(a.get(\"monument_uniformise\"))\n",
    "    if m_m == target_monument:\n",
    "        count_monu_manual += 1\n",
    "    if a_m == target_monument:\n",
    "        count_monu_auto += 1\n",
    "    if m_m == target_monument and a_m == target_monument:\n",
    "        count_monu_both += 1\n",
    "\n",
    "    m_c = normalize(m.get(\"city\"))\n",
    "    a_c = normalize(a.get(\"city\"))\n",
    "    if m_c == target_city:\n",
    "        count_city_manual += 1\n",
    "    if a_c == target_city:\n",
    "        count_city_auto += 1\n",
    "    if m_c == target_city and a_c == target_city:\n",
    "        count_city_both += 1\n",
    "\n",
    "print('5 villes pour lesquelles on a pas le nom de la ville sur la carte')\n",
    "for k in keys:\n",
    "    s = stats[k]\n",
    "    print(f\"{k}: total={s['total']} errors={s['errors']} \")\n",
    "\n",
    "print(\" \")\n",
    "print(f\"hamlet_uniformise == 'Aucun lieu-dit' (manual): {count_hamlet_manual}\")\n",
    "print(f\"hamlet_uniformise == 'Aucun lieu-dit' (auto): {count_hamlet_auto}\")\n",
    "print(f\"hamlet_uniformise == 'Aucun lieu-dit' (both): {count_hamlet_both}\")\n",
    "\n",
    "print(\" \")\n",
    "print(f\"monument_uniformise == 'Aucun monument' (manual): {count_monu_manual}\")\n",
    "print(f\"monument_uniformise == 'Aucun monument' (auto): {count_monu_auto}\")\n",
    "print(f\"monument_uniformise == 'Aucun monument' (both): {count_monu_both}\")\n",
    "\n",
    "print(\" \")\n",
    "print(f\"city == 'Aucune ville' (manual): {count_city_manual}\")\n",
    "print(f\"city == 'Aucune ville' (auto): {count_city_auto}\")\n",
    "print(f\"city == 'Aucune ville' (both): {count_city_both}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "086a7783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.9/site-packages (3.9.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.9/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.9/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.9/site-packages (from matplotlib) (4.60.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.9/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in ./.venv/lib/python3.9/site-packages (from matplotlib) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.9/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.9/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.9/site-packages (from matplotlib) (3.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.9/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./.venv/lib/python3.9/site-packages (from matplotlib) (6.5.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./.venv/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.23.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3e2bc85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pathlib\n",
      "  Using cached pathlib-1.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Using cached pathlib-1.0.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: pathlib\n",
      "Successfully installed pathlib-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1464d702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok -> data/out_vaucluse/plots\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "out_dir = Path(\"data/out_vaucluse/plots\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Données\n",
    "total = 84\n",
    "errors = {\n",
    "    \"city\": 4,\n",
    "    \"monument_uniformise\": 17,\n",
    "    \"hamlet_uniformise\": 7,\n",
    "}\n",
    "\n",
    "hamlet_aucun = {\n",
    "    \"manual\": 72,\n",
    "    \"auto\": 73,\n",
    "    \"both\": 70,\n",
    "}\n",
    "\n",
    "monu_aucun = {\n",
    "    \"manual\": 30,\n",
    "    \"auto\": 21,\n",
    "    \"both\": 21,\n",
    "}\n",
    "\n",
    "city_aucune = {\n",
    "    \"manual\": 5,  # <- remplace par tes valeurs\n",
    "    \"auto\": 5,    # <- remplace par tes valeurs\n",
    "    \"both\": 5,    # <- remplace par tes valeurs\n",
    "}\n",
    "\n",
    "# 1) Erreurs par champ\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(errors.keys(), errors.values(), color=[\"#4C78A8\", \"#F58518\", \"#54A24B\"])\n",
    "plt.title(\"Erreurs par champ (sur 84)\")\n",
    "plt.ylabel(\"Nombre d'erreurs\")\n",
    "plt.savefig(out_dir / \"errors_par_champ.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# 2) Hamlet: Aucun lieu-dit\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(hamlet_aucun.keys(), hamlet_aucun.values(), color=\"#72B7B2\")\n",
    "plt.title(\"hamlet_uniformise = 'Aucun lieu-dit'\")\n",
    "plt.ylabel(\"Nombre\")\n",
    "plt.savefig(out_dir / \"hamlet_aucun_lieu_dit.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# 3) Monument: Aucun monument\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(monu_aucun.keys(), monu_aucun.values(), color=\"#E45756\")\n",
    "plt.title(\"monument_uniformise = 'Aucun monument'\")\n",
    "plt.ylabel(\"Nombre\")\n",
    "plt.savefig(out_dir / \"monument_aucun.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# 4) City: Aucune ville\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(city_aucune.keys(), city_aucune.values(), color=\"#9D755D\")\n",
    "plt.title(\"city = 'Aucune ville'\")\n",
    "plt.ylabel(\"Nombre\")\n",
    "plt.savefig(out_dir / \"city_aucune_ville.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# 5) Taux d'erreurs (%)\n",
    "plt.figure(figsize=(6,4))\n",
    "rates = {k: (v / total) * 100 for k, v in errors.items()}\n",
    "plt.bar(rates.keys(), rates.values(), color=\"#B279A2\")\n",
    "plt.title(\"Taux d'erreur par champ (%)\")\n",
    "plt.ylabel(\"Pourcentage\")\n",
    "plt.savefig(out_dir / \"taux_erreurs.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(\"ok ->\", out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8493e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
