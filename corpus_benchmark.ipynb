{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd38f807",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_repo_benchmark = \"./data/corpus_benchmark\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70c9a47",
   "metadata": {},
   "source": [
    "### renommer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c45e0046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path_repo_benchmark = Path(\"./data/corpus_benchmark\")\n",
    "\n",
    "for p in path_repo_benchmark.glob(\"Copy of FRA*.jpg\"):\n",
    "    new_name = p.name.replace(\"Copy of \", \"\", 1)\n",
    "    target = p.with_name(new_name)\n",
    "    p.rename(target)\n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c7213c",
   "metadata": {},
   "source": [
    "### créer jsonl de validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c76a573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok -> data/corpus_benchmark.jsonl count: 50\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "path_repo_benchmark = Path(\"./data/corpus_benchmark\")\n",
    "out_path = Path(\"data/corpus_benchmark.jsonl\")\n",
    "\n",
    "items = []\n",
    "for p in sorted(path_repo_benchmark.glob(\"FRA*.jpg\")):\n",
    "    items.append({\n",
    "        \"nom_image\": p.name,\n",
    "        \"raw_text\": \"\",\n",
    "        \"city\": \"\",\n",
    "        \"hamlet\": \"\",\n",
    "        \"monument\": \"\",\n",
    "        \"other\": \"\"\n",
    "    })\n",
    "\n",
    "with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for obj in items:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"ok ->\", out_path, \"count:\", len(items))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9741ee7f",
   "metadata": {},
   "source": [
    "# corpus test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f6e461d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path = Path(\"data/test20/corpus_test20\")\n",
    "\n",
    "for p in path.iterdir():\n",
    "    if p.is_file() and p.name.startswith(\"copy_\"):\n",
    "        new_name = p.name.replace(\"copy_\", \"\", 1)\n",
    "        p.rename(p.with_name(new_name))\n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb39f36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok -> data/test20/metadata_test20.json rows: 20\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "path_meta = Path(\"data/metadata.json\")\n",
    "path_trans = Path(\"data/test20/transcriptions_coherentes_final.json\")\n",
    "out_path = Path(\"data/test20/metadata_test20.json\")\n",
    "\n",
    "meta = json.loads(path_meta.read_text(encoding=\"utf-8\"))\n",
    "trans = json.loads(path_trans.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "wanted = {t.get(\"file_name\") for t in trans if t.get(\"file_name\")}\n",
    "\n",
    "file_names = meta.get(\"File name\", [])\n",
    "keep_idx = [i for i, name in enumerate(file_names) if name in wanted]\n",
    "\n",
    "filtered = {}\n",
    "for key, values in meta.items():\n",
    "    if isinstance(values, list):\n",
    "        filtered[key] = [values[i] for i in keep_idx]\n",
    "    else:\n",
    "        filtered[key] = values\n",
    "\n",
    "out_path.write_text(json.dumps(filtered, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(\"ok ->\", out_path, \"rows:\", len(keep_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b65e814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok -> data/test20/metadata_test20_enrichi.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "path_meta = Path(\"data/test20/metadata_test20.json\")\n",
    "path_trans = Path(\"data/test20/transcriptions_coherentes_final.json\")\n",
    "out_path = Path(\"data/test20/metadata_test20_enrichi.json\")\n",
    "\n",
    "meta = json.loads(path_meta.read_text(encoding=\"utf-8\"))\n",
    "trans = json.loads(path_trans.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "# Index par nom de fichier\n",
    "trans_by_name = {t.get(\"file_name\"): t for t in trans}\n",
    "\n",
    "file_names = meta.get(\"File name\", [])\n",
    "n = len(file_names)\n",
    "\n",
    "# toutes les clés présentes dans transcriptions (sauf file_name)\n",
    "trans_keys = set()\n",
    "for t in trans:\n",
    "    for k in t.keys():\n",
    "        if k != \"file_name\":\n",
    "            trans_keys.add(k)\n",
    "\n",
    "# colonnes cibles (prefixées pour éviter collision)\n",
    "col_map = {k: f\"trans_{k}\" for k in sorted(trans_keys)}\n",
    "\n",
    "# initialise les colonnes\n",
    "for col in col_map.values():\n",
    "    if col not in meta:\n",
    "        meta[col] = [\"\"] * n\n",
    "\n",
    "# remplissage\n",
    "for i, fname in enumerate(file_names):\n",
    "    t = trans_by_name.get(fname)\n",
    "    if not t:\n",
    "        continue\n",
    "    for src_key, dst_key in col_map.items():\n",
    "        meta[dst_key][i] = t.get(src_key, \"\")\n",
    "\n",
    "out_path.write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(\"ok ->\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7420aefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok -> data/test20/metadata_test20_enrichi.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "in_path = Path(\"data/test20/metadata_test20_enrichi.json\")\n",
    "out_path = Path(\"data/test20/metadata_test20_enrichi.csv\")\n",
    "\n",
    "data = json.loads(in_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "fields = list(data.keys())\n",
    "n = len(data.get(\"File name\", []))\n",
    "\n",
    "with out_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(fields)\n",
    "    for i in range(n):\n",
    "        row = []\n",
    "        for k in fields:\n",
    "            v = data.get(k, \"\")\n",
    "            if isinstance(v, list):\n",
    "                row.append(v[i] if i < len(v) else \"\")\n",
    "            else:\n",
    "                row.append(v)\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"ok ->\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc023c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok -> data/metadata_monu_52.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "in_path = Path(\"data/metadata_monu.json\")\n",
    "out_path = Path(\"data/metadata_monu_52.json\")\n",
    "\n",
    "data = json.loads(in_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "# longueur basée sur File name\n",
    "n = min(52, len(data.get(\"File name\", [])))\n",
    "\n",
    "# supprimer colonnes\n",
    "data.pop(\"longitude\", None)\n",
    "data.pop(\"latitude\", None)\n",
    "\n",
    "# tronquer toutes les colonnes listes\n",
    "for k, v in data.items():\n",
    "    if isinstance(v, list):\n",
    "        data[k] = v[:n]\n",
    "\n",
    "out_path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(\"ok ->\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d8024f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok -> data/metadata_monu_52.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "in_path = Path(\"data/metadata_monu_52.json\")\n",
    "out_path = Path(\"data/metadata_monu_52.csv\")\n",
    "\n",
    "data = json.loads(in_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "fields = list(data.keys())\n",
    "n = len(data.get(\"File name\", []))\n",
    "\n",
    "with out_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(fields)\n",
    "    for i in range(n):\n",
    "        row = []\n",
    "        for k in fields:\n",
    "            v = data.get(k, \"\")\n",
    "            if isinstance(v, list):\n",
    "                row.append(v[i] if i < len(v) else \"\")\n",
    "            else:\n",
    "                row.append(v)\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"ok ->\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efc2a21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok -> data/metadata.json and data/metadata.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "path_json = Path(\"data/metadata.json\")\n",
    "path_csv = Path(\"data/metadata.csv\")\n",
    "\n",
    "remove_cols = {\n",
    "    \"City\", \"Raw text\", \"Hamlet\", \"Monument\", \"Other\", \"coordonnées\"\n",
    "}\n",
    "rename_from = \"code_insee\"\n",
    "rename_to = \"code_insee_commune\"\n",
    "\n",
    "# --- JSON ---\n",
    "data = json.loads(path_json.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "# remove columns\n",
    "for col in remove_cols:\n",
    "    data.pop(col, None)\n",
    "\n",
    "# rename column\n",
    "if rename_from in data:\n",
    "    data[rename_to] = data.pop(rename_from)\n",
    "\n",
    "path_json.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# --- CSV ---\n",
    "with path_csv.open(\"r\", encoding=\"utf-8\") as f_in:\n",
    "    reader = csv.DictReader(f_in)\n",
    "    fieldnames = []\n",
    "    for fn in reader.fieldnames:\n",
    "        if fn in remove_cols:\n",
    "            continue\n",
    "        if fn == rename_from:\n",
    "            fieldnames.append(rename_to)\n",
    "        else:\n",
    "            fieldnames.append(fn)\n",
    "\n",
    "    rows = []\n",
    "    for row in reader:\n",
    "        new_row = {}\n",
    "        for k, v in row.items():\n",
    "            if k in remove_cols:\n",
    "                continue\n",
    "            if k == rename_from:\n",
    "                new_row[rename_to] = v\n",
    "            else:\n",
    "                new_row[k] = v\n",
    "        rows.append(new_row)\n",
    "\n",
    "with path_csv.open(\"w\", newline=\"\", encoding=\"utf-8\") as f_out:\n",
    "    writer = csv.DictWriter(f_out, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(\"ok ->\", path_json, \"and\", path_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fad9abe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement PIL (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for PIL\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e15a5081",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12405cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.9.1-cp313-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.20.2-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.13/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.13/site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Collecting fsspec>=0.8.5 (from torch)\n",
      "  Using cached fsspec-2025.12.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.13/site-packages (from jinja2->torch) (3.0.3)\n",
      "Using cached torch-2.9.1-cp313-none-macosx_11_0_arm64.whl (74.5 MB)\n",
      "Using cached fsspec-2025.12.0-py3-none-any.whl (201 kB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Downloading filelock-3.20.2-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: mpmath, sympy, fsspec, filelock, torch\n",
      "\u001b[2K   \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [sympy]^C\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6bad22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
