{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56704ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_metadata = \"data/metadata.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04a0ab58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok -> data/metadata.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "path_metadata = Path(\"data/metadata.json\")  # adapte si besoin\n",
    "out_path = Path(\"data/metadata.csv\")\n",
    "\n",
    "data = json.loads(path_metadata.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "with out_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    # en-tête\n",
    "    writer.writerow([\"File name\"] + [k for k in data.keys() if k != \"File name\"])\n",
    "    # lignes\n",
    "    file_names = data.get(\"File name\", [])\n",
    "    n = len(file_names)\n",
    "    for i in range(n):\n",
    "        row = [data.get(\"File name\", [])[i]]\n",
    "        for k in data.keys():\n",
    "            if k == \"File name\":\n",
    "                continue\n",
    "            row.append(data.get(k, [None]*n)[i] if isinstance(data.get(k), list) else data.get(k))\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"ok ->\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b93423f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok -> data/metadata_insee.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import unicodedata\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "path_metadata = Path(\"./data/metadata.json\")\n",
    "dir_fantoir = Path(\"./data/fantoir_communes\")\n",
    "out_path = Path(\"./data/metadata_insee.json\")\n",
    "\n",
    "def normalize(s):\n",
    "    s = s or \"\"\n",
    "    s = s.lower()\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def slugify(s):\n",
    "    s = normalize(s)\n",
    "    s = s.replace(\"'\", \" \")\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \"_\", s).strip(\"_\")\n",
    "    return s\n",
    "\n",
    "def find_fantoir_file(city):\n",
    "    # ex: \"L'Isle-sur-la-Sorgue\" -> isle_sur_sorgue_fantoir.csv\n",
    "    base = slugify(city)\n",
    "    candidates = list(dir_fantoir.glob(f\"{base}*.csv\"))\n",
    "    if candidates:\n",
    "        return candidates[0]\n",
    "    # fallback: drop leading article (le/la/l)\n",
    "    for prefix in (\"le_\", \"la_\", \"l_\"):\n",
    "        if base.startswith(prefix):\n",
    "            base2 = base[len(prefix):]\n",
    "            candidates = list(dir_fantoir.glob(f\"{base2}*.csv\"))\n",
    "            if candidates:\n",
    "                return candidates[0]\n",
    "    return None\n",
    "\n",
    "def code_insee_from_fantoir(city, edifice):\n",
    "    path = find_fantoir_file(city)\n",
    "    if not path:\n",
    "        return None\n",
    "    target = normalize(edifice)\n",
    "    if not target:\n",
    "        return None\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            mot = normalize(row.get(\"mot_directeur\"))\n",
    "            if target in mot or mot in target:\n",
    "                ident = row.get(\"id\", \"\")\n",
    "                if ident and \"-\" in ident:\n",
    "                    return ident.split(\"-\", 1)[0]  # code INSEE\n",
    "    return None\n",
    "\n",
    "data = json.loads(path_metadata.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "# on gère \"Édifice\" ou \"édifice\"\n",
    "key_edifice = \"Édifice\" if \"Édifice\" in data else \"édifice\"\n",
    "\n",
    "file_names = data.get(\"File name\", [])\n",
    "n = len(file_names)\n",
    "\n",
    "codes = []\n",
    "for i in range(n):\n",
    "    edifice = data.get(key_edifice, [\"\"] * n)[i]\n",
    "    if not edifice:\n",
    "        codes.append(\"\")\n",
    "        continue\n",
    "    city = data.get(\"City\", [\"\"] * n)[i]\n",
    "    code = code_insee_from_fantoir(city, edifice)\n",
    "    codes.append(code or \"\")\n",
    "\n",
    "data[\"code_insee\"] = codes\n",
    "\n",
    "out_path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(\"ok ->\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd3bca87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok -> data/metadata_insee.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "in_path = Path(\"./data/metadata_insee.json\")\n",
    "out_path = Path(\"./data/metadata_insee.csv\")\n",
    "\n",
    "data = json.loads(in_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "# Les colonnes sont les clés du dict\n",
    "fields = list(data.keys())\n",
    "\n",
    "# Nombre de lignes basé sur \"File name\"\n",
    "n = len(data.get(\"File name\", []))\n",
    "\n",
    "with out_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(fields)\n",
    "    for i in range(n):\n",
    "        row = []\n",
    "        for k in fields:\n",
    "            v = data.get(k, \"\")\n",
    "            if isinstance(v, list):\n",
    "                row.append(v[i] if i < len(v) else \"\")\n",
    "            else:\n",
    "                row.append(v)\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"ok ->\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7113e56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok -> data/metadata_insee.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import unicodedata\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "path_metadata = Path(\"data/metadata.json\")\n",
    "dir_fantoir = Path(\"data/fantoir_communes\")\n",
    "out_path = Path(\"data/metadata_insee.csv\")\n",
    "\n",
    "def normalize(s):\n",
    "    s = (s or \"\").lower()\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def slugify(s):\n",
    "    s = normalize(s).replace(\"'\", \" \")\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \"_\", s).strip(\"_\")\n",
    "    return s\n",
    "\n",
    "def find_fantoir_file(city):\n",
    "    base = slugify(city)\n",
    "    candidates = list(dir_fantoir.glob(f\"{base}*.csv\"))\n",
    "    if candidates:\n",
    "        return candidates[0]\n",
    "    for prefix in (\"le_\", \"la_\", \"l_\"):\n",
    "        if base.startswith(prefix):\n",
    "            base2 = base[len(prefix):]\n",
    "            candidates = list(dir_fantoir.glob(f\"{base2}*.csv\"))\n",
    "            if candidates:\n",
    "                return candidates[0]\n",
    "    return None\n",
    "\n",
    "def code_insee_from_fantoir(city, edifice):\n",
    "    path = find_fantoir_file(city)\n",
    "    if not path:\n",
    "        return \"\"\n",
    "    target = normalize(edifice)\n",
    "    if not target:\n",
    "        return \"\"\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            mot = normalize(row.get(\"mot_directeur\"))\n",
    "            if target in mot or mot in target:\n",
    "                ident = row.get(\"id\", \"\")\n",
    "                if ident and \"-\" in ident:\n",
    "                    return ident.split(\"-\", 1)[0]\n",
    "    return \"\"\n",
    "\n",
    "data = json.loads(path_metadata.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "key_edifice = \"Édifice\" if \"Édifice\" in data else \"édifice\"\n",
    "file_names = data.get(\"File name\", [])\n",
    "n = len(file_names)\n",
    "\n",
    "fields = list(data.keys()) + [\"code_insee\"]\n",
    "\n",
    "with out_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(fields)\n",
    "    for i in range(n):\n",
    "        row = []\n",
    "        for k in data.keys():\n",
    "            v = data.get(k, \"\")\n",
    "            if isinstance(v, list):\n",
    "                row.append(v[i] if i < len(v) else \"\")\n",
    "            else:\n",
    "                row.append(v)\n",
    "        edifice = data.get(key_edifice, [\"\"] * n)[i]\n",
    "        city = data.get(\"City\", [\"\"] * n)[i]\n",
    "        code = code_insee_from_fantoir(city, edifice) if edifice else \"\"\n",
    "        row.append(code)\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"ok ->\", out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ed54c6",
   "metadata": {},
   "source": [
    "### gros échec je v tenter pour une seule ville"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ff1b27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_metadata = \"./data/metadata.json\"\n",
    "path_fantoir_apt = \"./data/fantoir_communes/apt_fantoir.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff01df4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok -> data/metadata.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import unicodedata\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "path_metadata = Path(\"data/metadata.json\")\n",
    "path_fantoir = Path(\"data/fantoir_communes/apt_fantoir.csv\")\n",
    "\n",
    "def normalize(s):\n",
    "    s = (s or \"\").lower()\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def find_code_insee_from_matiere(matiere):\n",
    "    target = normalize(matiere)\n",
    "    if not target:\n",
    "        return \"\"\n",
    "    with path_fantoir.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            mot = normalize(row.get(\"mot_directeur\"))\n",
    "            if mot == target:\n",
    "                ident = row.get(\"id\", \"\")\n",
    "                if ident and \"-\" in ident:\n",
    "                    return ident.split(\"-\", 1)[0]\n",
    "    return \"\"\n",
    "\n",
    "data = json.loads(path_metadata.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "key_matiere = \"Matière\" if \"Matière\" in data else \"matière\"\n",
    "file_names = data.get(\"File name\", [])\n",
    "n = len(file_names)\n",
    "\n",
    "codes = []\n",
    "for i in range(n):\n",
    "    city = data.get(\"City\", [\"\"] * n)[i]\n",
    "    matiere = data.get(key_matiere, [\"\"] * n)[i]\n",
    "    if city == \"Apt\" and matiere:\n",
    "        code = find_code_insee_from_matiere(matiere)\n",
    "        codes.append(code)\n",
    "    else:\n",
    "        codes.append(\"\")\n",
    "\n",
    "data[\"code_insee\"] = codes\n",
    "\n",
    "path_metadata.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(\"ok ->\", path_metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98c2c2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok -> data/metadata.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "path_metadata = Path(\"data/metadata.json\")  # adapte si besoin\n",
    "out_path = Path(\"data/metadata.csv\")\n",
    "\n",
    "data = json.loads(path_metadata.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "with out_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    # en-tête\n",
    "    writer.writerow([\"File name\"] + [k for k in data.keys() if k != \"File name\"])\n",
    "    # lignes\n",
    "    file_names = data.get(\"File name\", [])\n",
    "    n = len(file_names)\n",
    "    for i in range(n):\n",
    "        row = [data.get(\"File name\", [])[i]]\n",
    "        for k in data.keys():\n",
    "            if k == \"File name\":\n",
    "                continue\n",
    "            row.append(data.get(k, [None]*n)[i] if isinstance(data.get(k), list) else data.get(k))\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"ok ->\", out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606710c9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4678205c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok -> data/v_commune_2025_reduit.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "in_path = Path(\"data/v_commune_2025.csv\")\n",
    "out_path = Path(\"data/v_commune_2025_reduit.csv\")\n",
    "\n",
    "with in_path.open(\"r\", encoding=\"utf-8\") as f_in, out_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f_out:\n",
    "    reader = csv.DictReader(f_in)\n",
    "    writer = csv.DictWriter(f_out, fieldnames=[\"COM\", \"NCC\"])\n",
    "    writer.writeheader()\n",
    "    for row in reader:\n",
    "        writer.writerow({\"COM\": row.get(\"COM\", \"\"), \"NCC\": row.get(\"NCC\", \"\")})\n",
    "\n",
    "print(\"ok ->\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d6d6a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok -> data/v_commune_2025_reduit_84.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "in_path = Path(\"data/v_commune_2025_reduit.csv\")\n",
    "out_path = Path(\"data/v_commune_2025_reduit_84.csv\")\n",
    "\n",
    "with in_path.open(\"r\", encoding=\"utf-8\") as f_in, out_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f_out:\n",
    "    reader = csv.DictReader(f_in)\n",
    "    writer = csv.DictWriter(f_out, fieldnames=reader.fieldnames)\n",
    "    writer.writeheader()\n",
    "    for row in reader:\n",
    "        com = row.get(\"COM\", \"\")\n",
    "        if com.startswith(\"84\"):\n",
    "            writer.writerow(row)\n",
    "\n",
    "print(\"ok ->\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "757e0f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok -> data/metadata.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import unicodedata\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "path_metadata = Path(\"data/metadata.json\")\n",
    "path_communes = Path(\"data/v_commune_2025_reduit_84.csv\")\n",
    "\n",
    "def normalize(s):\n",
    "    s = (s or \"\").lower()\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def strip_parens(s):\n",
    "    return re.sub(r\"\\s*\\(.*?\\)\\s*\", \" \", s or \"\").strip()\n",
    "\n",
    "# Map commune name -> code INSEE\n",
    "name_to_code = {}\n",
    "with path_communes.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        name = normalize(row.get(\"NCC\"))\n",
    "        code = row.get(\"COM\", \"\")\n",
    "        if name and code:\n",
    "            name_to_code[name] = code\n",
    "\n",
    "data = json.loads(path_metadata.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "geo_names = data.get(\"Nom géographique\", [])\n",
    "n = len(geo_names)\n",
    "\n",
    "codes = []\n",
    "for i in range(n):\n",
    "    raw = geo_names[i]\n",
    "    cleaned = strip_parens(raw)\n",
    "    code = name_to_code.get(normalize(cleaned), \"\")\n",
    "    codes.append(code)\n",
    "\n",
    "data[\"code_insee\"] = codes\n",
    "\n",
    "path_metadata.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(\"ok ->\", path_metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f4888ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beaumes-de-Venise\n",
      "Beaumont-de-Pertuis\n",
      "Cabrières-d'Aigues\n",
      "Camaret-sur-Aigues\n",
      "Caumont-sur-Durance\n",
      "Châteauneuf-du-Pape\n",
      "Crillon-le-Brave\n",
      "Dentelles de Montmirail\n",
      "Entraigues-sur-la-Sorgue\n",
      "Fontaine-de-Vaucluse\n",
      "L'Isle-sur-la-Sorgue\n",
      "La Bastide-des-Jourdans\n",
      "La Bastidonne\n",
      "La Tour-d'Aigues\n",
      "Le Barroux\n",
      "Le Beaucet\n",
      "Le Pontet\n",
      "Le Thor\n",
      "Morières-lès-Avignon\n",
      "Pernes-les-Fontaines\n",
      "Peypin-d'Aigues\n",
      "Saint-Christol\n",
      "Saint-Didier\n",
      "Saint-Martin-de-Castillon\n",
      "Saint-Martin-de-la-Brasque\n",
      "Saint-Pantaléon\n",
      "Saint-Saturnin-lès-Apt\n",
      "Sainte-Cécile-les-Vignes\n",
      "Saumanes-de-Vaucluse\n",
      "Savoillans\n",
      "Sérignan-du-Comtat\n",
      "Vaison-la-Romaine\n",
      "Ventoux\n",
      "Villes-sur-Auzon\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "path_metadata = Path(\"data/metadata.json\")\n",
    "\n",
    "data = json.loads(path_metadata.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "geo = data.get(\"Nom géographique\", [])\n",
    "codes = data.get(\"code_insee\", [])\n",
    "\n",
    "missing = set()\n",
    "\n",
    "for i in range(len(geo)):\n",
    "    code = codes[i] if i < len(codes) else \"\"\n",
    "    if not code:\n",
    "        # nettoie les parenthèses pour afficher un nom propre\n",
    "        name = re.sub(r\"\\s*\\(.*?\\)\\s*\", \" \", geo[i]).strip()\n",
    "        if name:\n",
    "            missing.add(name)\n",
    "\n",
    "for name in sorted(missing):\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee32f768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78db2f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_code = [\"84012\", \"84014\", \"84024\",\"84029\",\"84034\",\"84037\",\"84041\",\"rien\",\"84043\",\"84139\",\"84054\",\"84009\",\"84010\",\"84133\",\"84008\",\"84011\",\"84092\",\"84132\",\"84081\",\"84088\",\"84090\",\"84107\",\"84108\",\"84112\",\"84113\",\"84114\",\"84118\",\"84106\",\"84124\",\"84125\",\"84127\",\"84137\",\"rien\",\"84148\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7c08cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(liste_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "845b08f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok -> data/metadata.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "path_metadata = Path(\"data/metadata.json\")\n",
    "\n",
    "villes = [\n",
    "    \"Beaumes-de-Venise\",\n",
    "    \"Beaumont-de-Pertuis\",\n",
    "    \"Cabrières-d'Aigues\",\n",
    "    \"Camaret-sur-Aigues\",\n",
    "    \"Caumont-sur-Durance\",\n",
    "    \"Châteauneuf-du-Pape\",\n",
    "    \"Crillon-le-Brave\",\n",
    "    \"Dentelles de Montmirail\",\n",
    "    \"Entraigues-sur-la-Sorgue\",\n",
    "    \"Fontaine-de-Vaucluse\",\n",
    "    \"L'Isle-sur-la-Sorgue\",\n",
    "    \"La Bastide-des-Jourdans\",\n",
    "    \"La Bastidonne\",\n",
    "    \"La Tour-d'Aigues\",\n",
    "    \"Le Barroux\",\n",
    "    \"Le Beaucet\",\n",
    "    \"Le Pontet\",\n",
    "    \"Le Thor\",\n",
    "    \"Morières-lès-Avignon\",\n",
    "    \"Pernes-les-Fontaines\",\n",
    "    \"Peypin-d'Aigues\",\n",
    "    \"Saint-Christol\",\n",
    "    \"Saint-Didier\",\n",
    "    \"Saint-Martin-de-Castillon\",\n",
    "    \"Saint-Martin-de-la-Brasque\",\n",
    "    \"Saint-Pantaléon\",\n",
    "    \"Saint-Saturnin-lès-Apt\",\n",
    "    \"Sainte-Cécile-les-Vignes\",\n",
    "    \"Saumanes-de-Vaucluse\",\n",
    "    \"Savoillans\",\n",
    "    \"Sérignan-du-Comtat\",\n",
    "    \"Vaison-la-Romaine\",\n",
    "    \"Ventoux\",\n",
    "    \"Villes-sur-Auzon\",\n",
    "]\n",
    "\n",
    "codes = [\n",
    "    \"84012\", \"84014\", \"84024\", \"84029\", \"84034\", \"84037\", \"84041\",\n",
    "    \"\", \"84043\", \"84139\", \"84054\", \"84009\", \"84010\", \"84133\", \"84008\",\n",
    "    \"84011\", \"84092\", \"84132\", \"84081\", \"84088\", \"84090\", \"84107\",\n",
    "    \"84108\", \"84112\", \"84113\", \"84114\", \"84118\", \"84106\", \"84124\",\n",
    "    \"84125\", \"84127\", \"84137\", \"\", \"84148\"\n",
    "]\n",
    "\n",
    "mapping = dict(zip(villes, codes))\n",
    "\n",
    "data = json.loads(path_metadata.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "geo = data.get(\"Nom géographique\", [])\n",
    "codes_insee = data.get(\"code_insee\", [\"\"] * len(geo))\n",
    "\n",
    "for i, raw in enumerate(geo):\n",
    "    if i < len(codes_insee) and codes_insee[i]:\n",
    "        continue  # déjà renseigné\n",
    "    name = re.sub(r\"\\s*\\(.*?\\)\\s*\", \" \", raw).strip()\n",
    "    code = mapping.get(name, \"\")\n",
    "    if code:\n",
    "        if i < len(codes_insee):\n",
    "            codes_insee[i] = code\n",
    "        else:\n",
    "            codes_insee.append(code)\n",
    "\n",
    "data[\"code_insee\"] = codes_insee\n",
    "\n",
    "path_metadata.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(\"ok ->\", path_metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b17a5da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok -> data/communes_vaucluse_coords.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "in_path = Path(\"./data/20230823-communes-departement-region.csv\")\n",
    "out_path = Path(\"./data/communes_vaucluse_coords.csv\")\n",
    "\n",
    "with in_path.open(\"r\", encoding=\"utf-8\") as f_in, out_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f_out:\n",
    "    reader = csv.DictReader(f_in)\n",
    "    writer = csv.DictWriter(f_out, fieldnames=[\"code_commune_INSEE\", \"longitude\", \"latitude\"])\n",
    "    writer.writeheader()\n",
    "    for row in reader:\n",
    "        code = row.get(\"code_commune_INSEE\", \"\")\n",
    "        if code.startswith(\"84\"):\n",
    "            writer.writerow({\n",
    "                \"code_commune_INSEE\": code,\n",
    "                \"longitude\": row.get(\"longitude\", \"\"),\n",
    "                \"latitude\": row.get(\"latitude\", \"\")\n",
    "            })\n",
    "\n",
    "print(\"ok ->\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ffc90a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok -> data/metadata.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "path_metadata = Path(\"data/metadata.json\")\n",
    "path_coords = Path(\"data/communes_vaucluse_coords.csv\")\n",
    "\n",
    "# Charge la table code_insee -> (lat, lon)\n",
    "coords = {}\n",
    "with path_coords.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    # accepte plusieurs variantes de nom de colonne\n",
    "    for row in reader:\n",
    "        code = (\n",
    "            row.get(\"code_commune_INSEE\")\n",
    "            or row.get(\"code_commune_insee\")\n",
    "            or row.get(\"COM\")\n",
    "            or \"\"\n",
    "        )\n",
    "        if code:\n",
    "            coords[code] = (row.get(\"latitude\", \"\"), row.get(\"longitude\", \"\"))\n",
    "\n",
    "data = json.loads(path_metadata.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "codes = data.get(\"code_insee\", [])\n",
    "n = len(codes)\n",
    "\n",
    "latitudes = []\n",
    "longitudes = []\n",
    "\n",
    "for i in range(n):\n",
    "    code = codes[i]\n",
    "    lat, lon = coords.get(code, (\"\", \"\"))\n",
    "    latitudes.append(lat)\n",
    "    longitudes.append(lon)\n",
    "\n",
    "data[\"latitude\"] = latitudes\n",
    "data[\"longitude\"] = longitudes\n",
    "\n",
    "path_metadata.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(\"ok ->\", path_metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c70ac2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6266e3fa",
   "metadata": {},
   "source": [
    "### on va tenter avec librairie osmnx pour monuments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "280ddc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting osmnx\n",
      "  Downloading osmnx-2.0.7-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting geopandas>=1.0.1 (from osmnx)\n",
      "  Downloading geopandas-1.1.2-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting networkx>=2.5 (from osmnx)\n",
      "  Downloading networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: numpy>=1.22 in ./.venv/lib/python3.13/site-packages (from osmnx) (2.4.0)\n",
      "Requirement already satisfied: pandas>=1.4 in ./.venv/lib/python3.13/site-packages (from osmnx) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.27 in ./.venv/lib/python3.13/site-packages (from osmnx) (2.32.5)\n",
      "Collecting shapely>=2.0 (from osmnx)\n",
      "  Using cached shapely-2.1.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting pyogrio>=0.7.2 (from geopandas>=1.0.1->osmnx)\n",
      "  Downloading pyogrio-0.12.1-cp313-cp313-macosx_12_0_arm64.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.13/site-packages (from geopandas>=1.0.1->osmnx) (25.0)\n",
      "Collecting pyproj>=3.5.0 (from geopandas>=1.0.1->osmnx)\n",
      "  Downloading pyproj-3.7.2-cp313-cp313-macosx_14_0_arm64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.13/site-packages (from pandas>=1.4->osmnx) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.13/site-packages (from pandas>=1.4->osmnx) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.13/site-packages (from pandas>=1.4->osmnx) (2025.3)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.13/site-packages (from pyogrio>=0.7.2->geopandas>=1.0.1->osmnx) (2026.1.4)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas>=1.4->osmnx) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests>=2.27->osmnx) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests>=2.27->osmnx) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests>=2.27->osmnx) (2.6.2)\n",
      "Downloading osmnx-2.0.7-py3-none-any.whl (101 kB)\n",
      "Downloading geopandas-1.1.2-py3-none-any.whl (341 kB)\n",
      "Downloading networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyogrio-0.12.1-cp313-cp313-macosx_12_0_arm64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyproj-3.7.2-cp313-cp313-macosx_14_0_arm64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached shapely-2.1.2-cp313-cp313-macosx_11_0_arm64.whl (1.6 MB)\n",
      "Installing collected packages: shapely, pyproj, pyogrio, networkx, geopandas, osmnx\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [osmnx]32m5/6\u001b[0m [osmnx]das]\n",
      "\u001b[1A\u001b[2KSuccessfully installed geopandas-1.1.2 networkx-3.6.1 osmnx-2.0.7 pyogrio-0.12.1 pyproj-3.7.2 shapely-2.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install osmnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c744669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03844e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Geocode édifices: 100%|██████████| 50/50 [00:50<00:00,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok -> data/metadata_monu.json and data/metadata_monu.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import time\n",
    "from pathlib import Path\n",
    "import osmnx as ox\n",
    "from tqdm import tqdm\n",
    "\n",
    "path_metadata = Path(\"data/metadata.json\")\n",
    "out_json = Path(\"data/metadata_monu.json\")\n",
    "out_csv = Path(\"data/metadata_monu.csv\")\n",
    "\n",
    "data = json.loads(path_metadata.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "edifices = data.get(\"Édifice\", [])\n",
    "cities = data.get(\"City\", [])\n",
    "geo_names = data.get(\"Nom géographique\", [])\n",
    "lat_communes = data.get(\"latitude\", [])\n",
    "lon_communes = data.get(\"longitude\", [])\n",
    "\n",
    "n = len(edifices)\n",
    "\n",
    "coords = []\n",
    "count_done = 0\n",
    "max_edifices = 50\n",
    "\n",
    "with tqdm(total=max_edifices, desc=\"Geocode édifices\") as pbar:\n",
    "    for i in range(n):\n",
    "        edifice = edifices[i] if i < len(edifices) else \"\"\n",
    "        if not edifice:\n",
    "            coords.append(\"\")\n",
    "            continue\n",
    "\n",
    "        if count_done >= max_edifices:\n",
    "            coords.append(\"\")  # ne pas géocoder au-delà de 50\n",
    "            continue\n",
    "\n",
    "        city = cities[i] if i < len(cities) else \"\"\n",
    "        if not city:\n",
    "            city = geo_names[i] if i < len(geo_names) else \"\"\n",
    "\n",
    "        if city:\n",
    "            query = f\"{edifice}, {city}, Vaucluse, France\"\n",
    "        else:\n",
    "            query = f\"{edifice}, Vaucluse, France\"\n",
    "\n",
    "        try:\n",
    "            lat, lon = ox.geocode(query)\n",
    "            coords.append(f\"{lat}, {lon}\")\n",
    "        except Exception:\n",
    "            lat_c = lat_communes[i] if i < len(lat_communes) else \"\"\n",
    "            lon_c = lon_communes[i] if i < len(lon_communes) else \"\"\n",
    "            coords.append(f\"{lat_c}, {lon_c}\" if lat_c and lon_c else \"\")\n",
    "\n",
    "        count_done += 1\n",
    "        pbar.update(1)\n",
    "        time.sleep(1)\n",
    "\n",
    "data[\"coordonnées\"] = coords\n",
    "\n",
    "out_json.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "fields = list(data.keys())\n",
    "with out_csv.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(fields)\n",
    "    for i in range(n):\n",
    "        row = []\n",
    "        for k in fields:\n",
    "            v = data.get(k, \"\")\n",
    "            if isinstance(v, list):\n",
    "                row.append(v[i] if i < len(v) else \"\")\n",
    "            else:\n",
    "                row.append(v)\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"ok ->\", out_json, \"and\", out_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2a3bacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Geocode édifices: 100%|██████████| 20/20 [00:30<00:00,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok -> data/test20/metadata_test20_monu.json and data/test20/metadata_test20_monu.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import time\n",
    "from pathlib import Path\n",
    "import osmnx as ox\n",
    "from tqdm import tqdm\n",
    "\n",
    "path_metadata = Path(\"data/test20/metadata_test20_enrichi.json\")\n",
    "out_json = Path(\"data/test20/metadata_test20_monu.json\")\n",
    "out_csv = Path(\"data/test20/metadata_test20_monu.csv\")\n",
    "\n",
    "data = json.loads(path_metadata.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "edifices = data.get(\"Édifice\", [])\n",
    "cities = data.get(\"City\", [])\n",
    "geo_names = data.get(\"Nom géographique\", [])\n",
    "lat_communes = data.get(\"latitude\", [])\n",
    "lon_communes = data.get(\"longitude\", [])\n",
    "\n",
    "n = len(edifices)\n",
    "\n",
    "coords = []\n",
    "with tqdm(total=n, desc=\"Geocode édifices\") as pbar:\n",
    "    for i in range(n):\n",
    "        edifice = edifices[i] if i < len(edifices) else \"\"\n",
    "        if not edifice:\n",
    "            coords.append(\"\")\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "\n",
    "        city = cities[i] if i < len(cities) else \"\"\n",
    "        if not city:\n",
    "            city = geo_names[i] if i < len(geo_names) else \"\"\n",
    "\n",
    "        if city:\n",
    "            query = f\"{edifice}, {city}, Vaucluse, France\"\n",
    "        else:\n",
    "            query = f\"{edifice}, Vaucluse, France\"\n",
    "\n",
    "        try:\n",
    "            lat, lon = ox.geocode(query)\n",
    "            coords.append(f\"{lat}, {lon}\")\n",
    "        except Exception:\n",
    "            lat_c = lat_communes[i] if i < len(lat_communes) else \"\"\n",
    "            lon_c = lon_communes[i] if i < len(lon_communes) else \"\"\n",
    "            coords.append(f\"{lat_c}, {lon_c}\" if lat_c and lon_c else \"\")\n",
    "\n",
    "        pbar.update(1)\n",
    "        time.sleep(1)\n",
    "\n",
    "data[\"coordonnées\"] = coords\n",
    "\n",
    "out_json.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "fields = list(data.keys())\n",
    "with out_csv.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(fields)\n",
    "    for i in range(n):\n",
    "        row = []\n",
    "        for k in fields:\n",
    "            v = data.get(k, \"\")\n",
    "            if isinstance(v, list):\n",
    "                row.append(v[i] if i < len(v) else \"\")\n",
    "            else:\n",
    "                row.append(v)\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"ok ->\", out_json, \"and\", out_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8149d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Geocode: 100%|██████████| 20/20 [00:23<00:00,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok -> data/test20/metadata_test20_coor.json and data/test20/metadata_test20_coor.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import time\n",
    "from pathlib import Path\n",
    "import osmnx as ox\n",
    "from tqdm import tqdm\n",
    "\n",
    "path_metadata = Path(\"data/test20/metadata_test20.json\")\n",
    "out_json = Path(\"data/test20/metadata_test20_coor.json\")\n",
    "out_csv = Path(\"data/test20/metadata_test20_coor.csv\")\n",
    "\n",
    "data = json.loads(path_metadata.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "geo_names = data.get(\"Nom géographique\", [])\n",
    "hamlets = data.get(\"trans_hamlet_uniformise\", [])\n",
    "edifices = data.get(\"Édifice\", [])\n",
    "monuments = data.get(\"trans_monument_uniformise\", [])\n",
    "cities = data.get(\"City\", [])\n",
    "\n",
    "n = len(geo_names)\n",
    "\n",
    "coords = []\n",
    "used_source = []\n",
    "\n",
    "with tqdm(total=n, desc=\"Geocode\") as pbar:\n",
    "    for i in range(n):\n",
    "        city = cities[i] if i < len(cities) else \"\"\n",
    "        candidates = [\n",
    "            (\"Nom géographique\", geo_names[i] if i < len(geo_names) else \"\"),\n",
    "            (\"trans_hamlet_uniformise\", hamlets[i] if i < len(hamlets) else \"\"),\n",
    "            (\"Édifice\", edifices[i] if i < len(edifices) else \"\"),\n",
    "            (\"trans_monument_uniformise\", monuments[i] if i < len(monuments) else \"\"),\n",
    "        ]\n",
    "\n",
    "        result = \"\"\n",
    "        source = \"\"\n",
    "        for label, value in candidates:\n",
    "            if not value:\n",
    "                continue\n",
    "            if city:\n",
    "                query = f\"{value}, {city}, Vaucluse, France\"\n",
    "            else:\n",
    "                query = f\"{value}, Vaucluse, France\"\n",
    "            try:\n",
    "                lat, lon = ox.geocode(query)\n",
    "                result = f\"{lat}, {lon}\"\n",
    "                source = label\n",
    "                break\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        coords.append(result)\n",
    "        used_source.append(source)\n",
    "        pbar.update(1)\n",
    "        time.sleep(1)\n",
    "\n",
    "data[\"coordonnées\"] = coords\n",
    "data[\"coord_source\"] = used_source  # optionnel: savoir quelle colonne a servi\n",
    "\n",
    "out_json.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "fields = list(data.keys())\n",
    "with out_csv.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(fields)\n",
    "    for i in range(n):\n",
    "        row = []\n",
    "        for k in fields:\n",
    "            v = data.get(k, \"\")\n",
    "            if isinstance(v, list):\n",
    "                row.append(v[i] if i < len(v) else \"\")\n",
    "            else:\n",
    "                row.append(v)\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"ok ->\", out_json, \"and\", out_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd6fa1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok -> data/test20/metadata_test20_coor.json and data/test20/metadata_test20_coor.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "json_path = Path(\"data/test20/metadata_test20_coor.json\")\n",
    "csv_path = Path(\"data/test20/metadata_test20_coor.csv\")\n",
    "\n",
    "# JSON\n",
    "data = json.loads(json_path.read_text(encoding=\"utf-8\"))\n",
    "data.pop(\"latitude\", None)\n",
    "data.pop(\"longitude\", None)\n",
    "json_path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# CSV\n",
    "with csv_path.open(\"r\", encoding=\"utf-8\") as f_in:\n",
    "    reader = csv.DictReader(f_in)\n",
    "    fieldnames = [fn for fn in reader.fieldnames if fn not in (\"latitude\", \"longitude\")]\n",
    "    rows = [{k: v for k, v in row.items() if k in fieldnames} for row in reader]\n",
    "\n",
    "with csv_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f_out:\n",
    "    writer = csv.DictWriter(f_out, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(\"ok ->\", json_path, \"and\", csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1e34b267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Geocode: 100%|██████████| 20/20 [00:20<00:00,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok -> data/test20/metadata_test20_coor.json and data/test20/metadata_test20_coor.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import time\n",
    "from pathlib import Path\n",
    "import osmnx as ox\n",
    "from tqdm import tqdm\n",
    "\n",
    "path_metadata = Path(\"data/test20/metadata_test20.json\")\n",
    "out_json = Path(\"data/test20/metadata_test20_coor.json\")\n",
    "out_csv = Path(\"data/test20/metadata_test20_coor.csv\")\n",
    "\n",
    "data = json.loads(path_metadata.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "geo_names = data.get(\"Nom géographique\", [])\n",
    "hamlets = data.get(\"trans_hamlet_uniformise\", [])\n",
    "edifices = data.get(\"Édifice\", [])\n",
    "monuments = data.get(\"trans_monument_uniformise\", [])\n",
    "cities = data.get(\"City\", [])\n",
    "\n",
    "n = len(geo_names)\n",
    "\n",
    "coords = []\n",
    "used_source = []\n",
    "\n",
    "with tqdm(total=n, desc=\"Geocode\") as pbar:\n",
    "    for i in range(n):\n",
    "        city = cities[i] if i < len(cities) else \"\"\n",
    "        # ordre de précision décroissante\n",
    "        candidates = [\n",
    "            (\"trans_monument_uniformise\", monuments[i] if i < len(monuments) else \"\"),\n",
    "            (\"Édifice\", edifices[i] if i < len(edifices) else \"\"),\n",
    "            (\"trans_hamlet_uniformise\", hamlets[i] if i < len(hamlets) else \"\"),\n",
    "            (\"Nom géographique\", geo_names[i] if i < len(geo_names) else \"\"),\n",
    "        ]\n",
    "\n",
    "        result = \"\"\n",
    "        source = \"\"\n",
    "        for label, value in candidates:\n",
    "            if not value:\n",
    "                continue\n",
    "            if city:\n",
    "                query = f\"{value}, {city}, Vaucluse, France\"\n",
    "            else:\n",
    "                query = f\"{value}, Vaucluse, France\"\n",
    "            try:\n",
    "                lat, lon = ox.geocode(query)\n",
    "                result = f\"{lat}, {lon}\"\n",
    "                source = label\n",
    "                break\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        coords.append(result)\n",
    "        used_source.append(source)\n",
    "        pbar.update(1)\n",
    "        time.sleep(1)\n",
    "\n",
    "data[\"coordonnées\"] = coords\n",
    "data[\"coord_source\"] = used_source\n",
    "\n",
    "# Supprime latitude/longitude si présents\n",
    "data.pop(\"latitude\", None)\n",
    "data.pop(\"longitude\", None)\n",
    "\n",
    "out_json.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "fields = [k for k in data.keys()]\n",
    "with out_csv.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(fields)\n",
    "    for i in range(n):\n",
    "        row = []\n",
    "        for k in fields:\n",
    "            v = data.get(k, \"\")\n",
    "            if isinstance(v, list):\n",
    "                row.append(v[i] if i < len(v) else \"\")\n",
    "            else:\n",
    "                row.append(v)\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"ok ->\", out_json, \"and\", out_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e26db7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Geocode: 100%|██████████| 20/20 [00:49<00:00,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok -> data/test20/metadata_test20_coor.json and data/test20/metadata_test20_coor.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import time\n",
    "from pathlib import Path\n",
    "import osmnx as ox\n",
    "from tqdm import tqdm\n",
    "\n",
    "path_metadata = Path(\"data/test20/metadata_test20_enrichi.json\")\n",
    "out_json = Path(\"data/test20/metadata_test20_coor.json\")\n",
    "out_csv = Path(\"data/test20/metadata_test20_coor.csv\")\n",
    "\n",
    "data = json.loads(path_metadata.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "geo_names = data.get(\"Nom géographique\", [])\n",
    "hamlets = data.get(\"trans_hamlet_uniformise\", [])\n",
    "edifices = data.get(\"Édifice\", [])\n",
    "monuments = data.get(\"trans_monument_uniformise\", [])\n",
    "cities = data.get(\"City\", [])\n",
    "\n",
    "n = len(geo_names)\n",
    "\n",
    "coords = []\n",
    "used_source = []\n",
    "\n",
    "with tqdm(total=n, desc=\"Geocode\") as pbar:\n",
    "    for i in range(n):\n",
    "        city = cities[i] if i < len(cities) else \"\"\n",
    "        candidates = [\n",
    "            (\"trans_monument_uniformise\", monuments[i] if i < len(monuments) else \"\"),\n",
    "            (\"Édifice\", edifices[i] if i < len(edifices) else \"\"),\n",
    "            (\"trans_hamlet_uniformise\", hamlets[i] if i < len(hamlets) else \"\"),\n",
    "            (\"Nom géographique\", geo_names[i] if i < len(geo_names) else \"\"),\n",
    "        ]\n",
    "\n",
    "        result = \"\"\n",
    "        source = \"\"\n",
    "        for label, value in candidates:\n",
    "            if not value:\n",
    "                continue\n",
    "            if city:\n",
    "                query = f\"{value}, {city}, Vaucluse, France\"\n",
    "            else:\n",
    "                query = f\"{value}, Vaucluse, France\"\n",
    "            try:\n",
    "                lat, lon = ox.geocode(query)\n",
    "                result = f\"{lat}, {lon}\"\n",
    "                source = label\n",
    "                break\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        coords.append(result)\n",
    "        used_source.append(source)\n",
    "        pbar.update(1)\n",
    "        time.sleep(1)\n",
    "\n",
    "data[\"coordonnées\"] = coords\n",
    "data[\"coord_source\"] = used_source\n",
    "\n",
    "# Supprime latitude/longitude si présents\n",
    "data.pop(\"latitude\", None)\n",
    "data.pop(\"longitude\", None)\n",
    "\n",
    "out_json.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "fields = list(data.keys())\n",
    "with out_csv.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(fields)\n",
    "    for i in range(n):\n",
    "        row = []\n",
    "        for k in fields:\n",
    "            v = data.get(k, \"\")\n",
    "            if isinstance(v, list):\n",
    "                row.append(v[i] if i < len(v) else \"\")\n",
    "            else:\n",
    "                row.append(v)\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"ok ->\", out_json, \"and\", out_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59213b31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6cd469",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
